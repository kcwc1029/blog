{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 展示如何使用 OpenAI 的 GPT 模型與 Llama 模型處理技術問題，特別是結合流式傳輸與靜態回應的展示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import ollama\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# 用於在 Jupyter Notebook 中實時顯示與更新 Markdown 格式的內容。\n",
    "from IPython.display import Markdown, display, update_display\n",
    "\n",
    "\n",
    "# NOTE: 選擇模型與加載 API 金鑰\n",
    "MODEL_GPT = 'gpt-4o-mini'  # 選擇 GPT 模型\n",
    "MODEL_LLAMA = 'llama3.2'   # 選擇 Llama 模型\n",
    "\n",
    "# 載入環境變數\n",
    "load_dotenv()\n",
    "openai = OpenAI()\n",
    "\n",
    "\n",
    "# NOTE: 定義問題與系統提示\n",
    "# 問題內容\n",
    "question = \"\"\"\n",
    "請解釋這段程式碼的作用和原因：\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "# 系統提示\n",
    "system_prompt = \"您是一位樂於助人的技術導師，回答有關 python 代碼、軟體工程、數據科學和法學碩士的問題\"\n",
    "# 用戶輸入提示\n",
    "user_prompt = \"請詳細解釋以下問題：\" + question\n",
    "\n",
    "# 組裝對話訊息\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "這段程式碼在 Python 中使用了 `yield from`，這是一個生成器的語法，用來從一個可迭代的對象中逐個返回元素。在這個特定的代碼片段中，`yield from` 後面使用了一個集合推導式（set comprehension）來生成一個集合。\n",
       "\n",
       "我們來逐部分解析這段程式碼的作用：\n",
       "\n",
       "1. **集合推導式**：`{book.get(\"author\") for book in books if book.get(\"author\")}`  \n",
       "   - `books` 是一個可迭代對象（例如列表），包含許多書籍的資料，通常這些資料以字典的形式存儲。\n",
       "   - `book.get(\"author\")` 會從每個 `book` 字典中提取出 `\"author\"` 鍵對應的值。\n",
       "   - `if book.get(\"author\")` 是一個條件語句，確保只有在 `book` 字典中存在 `\"author\"` 這一鍵，且其值不為 `None` 或空字符串的情況下，才會將這個作者名添加到集合中。\n",
       "   - 最終，這部分代碼會生成一個集合，這個集合只包含 `books` 中所有有效的作者名（即排除掉沒有作者的書籍）。\n",
       "\n",
       "2. **`yield from`**：  \n",
       "   - `yield from` 的作用是簡化從子生成器（也就是可迭代對象，如列表或集合）中返回值的過程。\n",
       "   - 在這裡，`yield from` 將會對上面生成的集合進行迭代，並逐個返回集合中的每一個作者名。\n",
       "\n",
       "### 總結\n",
       "因此，整段程式碼的作用是：從一組書籍中提取所有有效的作者名，並將它們逐個返回作為生成器的輸出。這樣的做法有以下幾個好處：\n",
       "\n",
       "- **去重**：因為集合會自動去除重複的作者名。\n",
       "- **懶加載**：使用生成器可以提高性能，特別是在處理大量數據時，因為它會在需要時才生成下一个值，而不是一次性生成所有值。\n",
       "\n",
       "示例用法可能如下：\n",
       "python\n",
       "def authors_generator(books):\n",
       "    yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book 2\", \"author\": \"Author B\"},\n",
       "    {\"title\": \"Book 3\", \"author\": \"Author A\"},  # 重複的作者\n",
       "    {\"title\": \"Book 4\"}  # 沒有作者\n",
       "    # 其他書籍...\n",
       "]\n",
       "\n",
       "for author in authors_generator(books):\n",
       "    print(author)\n",
       "\n",
       "這段代碼將會輸出 `Author A` 和 `Author B`，並且不會重複列印同一個作者名。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: GPT 模型的流式請求\n",
    "# 創建流式請求\n",
    "stream = openai.chat.completions.create(model=MODEL_GPT, messages=messages, stream=True)\n",
    "\n",
    "# 初始化回應變數\n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "# 處理流式數據\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or ''\n",
    "    response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")  # 清理多餘格式\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "我們來一步步分析這段程式碼：\n",
       "\n",
       "```python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "```\n",
       "\n",
       "這是一個 Python 3.3+ 的語法，使用了 `yield from` 的機制。\n",
       "\n",
       "**什么是 yield from ?**\n",
       "\n",
       "`yield from` 是一個可以讓我們將另一個 iterator 選項的 iterator 將其內容轉換成 generator 的 keyword。它的主要目的是使我們能夠透過其他 iterator 進行 iteration，從而減少 code 重複。\n",
       "\n",
       "**分解這段程式碼**\n",
       "\n",
       "現在，讓我們一步步分解這段程式碼：\n",
       "\n",
       "1. `{book.get(\"author\") for book in books if book.get(\"author\")}` 是一個 generator expression。\n",
       "\t* `for book in books` 進行 iteration 在 `books` 中找尋符合條件的 book。\n",
       "\t* `if book.get(\"author\")` 會對每個 book 进行 filters，從而保留包含 \"author\" 欄位的書籍。\n",
       "2. `yield from` 這個 keyword 則將該 generator expression 的 iterator 將其內容轉換成另一個 generator。\n",
       "3. 這樣做的結果是，我們得到了一個新的 iterator，它會透過所有符合條件的 book 找尋其 \"author\" 欄位的值。\n",
       "\n",
       "**目的和原因**\n",
       "\n",
       "這段程式碼的目的是將 book 的 \"author\" 欄位值轉換成一個 generator iterable。這通常會被用於下面幾種情況：\n",
       "\n",
       "* 這些值可能需要透過某個 iterator 进行處理或過濾。\n",
       "* 這些值可能需要透過另一個生成器進行 iteration。\n",
       "* 這樣做的結果是，程式碼更簡潔、更可讀。\n",
       "\n",
       "例如，如果我們有以下 `books` list：\n",
       "\n",
       "```python\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book 2\", \"author\": None},\n",
       "    {\"title\": \"Book 3\", \"author\": \"Author C\"}\n",
       "]\n",
       "```\n",
       "\n",
       "透過這段程式碼，我們可以得到一個 iterator，從而對每個 book 找尋其 \"author\" 欄位的值。這樣做的結果是：\n",
       "\n",
       "```python\n",
       "result = {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "print(result)  # Output: {\"Author A\", \"Author C\"}\n",
       "```\n",
       "\n",
       "總之，這段程式碼通過使用 `yield from` 來將 generator expression 的 iterator 將其內容轉換成另一個 generator，從而簡化程式碼並提高效率。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Llama 模型的回應處理\n",
    "# 使用 Llama 模型進行回應\n",
    "response = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
    "\n",
    "# 提取回應內容\n",
    "reply = response['message']['content']\n",
    "\n",
    "# 顯示回應\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定API_KET跟使用\n",
    "- 要先把API_KET放到.env\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "# import anthropic # cloud\n",
    "# import google.generativeai # gemini\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 載入環境便數\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "# google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# TODO: 連線到模型\n",
    "openai = OpenAI()\n",
    "# claude = anthropic.Anthropic()\n",
    "# google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嘗試讓大型語言模型講笑話"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"你是一個擅長講笑話的助手\"\n",
    "user_prompt = \"為數據科學家觀眾講一個輕鬆的笑話\"\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 調用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "為數據科學家觀眾講一個笑話：\n",
      "\n",
      "為什麼數據科學家不喜歡去海灘放鬆呢？因為他們總是在思考如何把海浪波形轉換成數據模型！\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "為數據科學家講的笑話：\n",
      "\n",
      "為什麼數據科學家總是帶著一把鋸子？\n",
      "\n",
      "因為他們想要\"切割\"數據！\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    # temperature=0.7 # 控制生成內容的隨機性(0-1)\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "為什麼數據科學家總是帶著眼鏡？\n",
      "\n",
      "因為他們需要用 \"R\" 看清東西！\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    # temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "\n",
    "# message = claude.messages.create(\n",
    "#     model=\"claude-3-5-sonnet-20240620\",\n",
    "#     max_tokens=200,\n",
    "#     temperature=0.7,\n",
    "#     system=system_message,\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": user_prompt},\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemini\n",
    "\n",
    "# gemini = google.generativeai.GenerativeModel(\n",
    "#     model_name='gemini-1.5-flash',\n",
    "#     system_instruction=system_message\n",
    "# )\n",
    "# response = gemini.generate_content(user_prompt)\n",
    "# print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用stream形式調用gpt-4o\n",
    "tream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot 多輪對話結構設計"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 對話歷史是以 列表形式 表示，每個元素是一個字典 (JSON 格式)。\n",
    "- 每個字典包含兩個關鍵屬性：\n",
    "    - role：表示發言角色，例如 system(設定模型的上下文)、user 或 assistant(模型自身)。\n",
    "    - content：角色發送的具體訊息內容。\n",
    "- 範例\n",
    "```json\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"It's sunny and warm.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about tomorrow?\"}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 模擬對話\u001b[39;00m\n\u001b[0;32m     36\u001b[0m gpt_reply \u001b[38;5;241m=\u001b[39m call_gpt()\n\u001b[1;32m---> 37\u001b[0m ollama_reply \u001b[38;5;241m=\u001b[39m \u001b[43mcall_ollama\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT Reply:\u001b[39m\u001b[38;5;124m\"\u001b[39m, gpt_reply)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama Reply:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ollama_reply)\n",
      "Cell \u001b[1;32mIn[15], line 32\u001b[0m, in \u001b[0;36mcall_ollama\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: gpt})\n\u001b[0;32m     31\u001b[0m     messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: ollama})\n\u001b[1;32m---> 32\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m(model\u001b[38;5;241m=\u001b[39mollama_model, messages\u001b[38;5;241m=\u001b[39mmessages)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'chat'"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "# 定義 GPT 和 Ollama 模型\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "ollama_model = \"llama3.2\"\n",
    "\n",
    "# 定義 GPT 和 Ollama 的角色\n",
    "gpt_system = \"你是一個非常好爭論的聊天機器人；你不同意談話中的任何事情，並對一切提出質疑。\"\n",
    "ollama_system = \"你是一個非常有禮貌、有禮貌的聊天機器人。你試著同意對方所說的一切，或找到共同點。如果對方不高興，你試著讓他們平靜下來並繼續聊天。\"\n",
    "\n",
    "# 定義初始訊息\n",
    "gpt_messages = [\"Hi there\"]\n",
    "ollama_messages = [\"Hi\"]\n",
    "\n",
    "# 定義 GPT 回應函數\n",
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, ollama in zip(gpt_messages, ollama_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": ollama})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# 定義 Ollama 回應函數\n",
    "def call_ollama():\n",
    "    messages = [{\"role\": \"system\", \"content\": ollama_system}]\n",
    "    for gpt, ollama in zip(gpt_messages, ollama_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": ollama})\n",
    "    response = ollama.chat(model=ollama_model, messages=messages)\n",
    "    return response['message']['content']\n",
    "\n",
    "# 模擬對話\n",
    "gpt_reply = call_gpt()\n",
    "ollama_reply = call_ollama()\n",
    "\n",
    "print(\"GPT Reply:\", gpt_reply)\n",
    "print(\"Ollama Reply:\", ollama_reply)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
